{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10138538,"sourceType":"datasetVersion","datasetId":6257190}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install open_clip_torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:10:34.245688Z","iopub.execute_input":"2024-12-08T22:10:34.246467Z","iopub.status.idle":"2024-12-08T22:10:43.946866Z","shell.execute_reply.started":"2024-12-08T22:10:34.246428Z","shell.execute_reply":"2024-12-08T22:10:43.946001Z"}},"outputs":[{"name":"stdout","text":"Collecting open_clip_torch\n  Downloading open_clip_torch-2.29.0-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.19.0)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2024.5.15)\nCollecting ftfy (from open_clip_torch)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (4.66.4)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.26.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.4.5)\nRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (1.0.11)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (2024.6.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy->open_clip_torch) (0.2.13)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (2.32.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->open_clip_torch) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->open_clip_torch) (10.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->open_clip_torch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\nDownloading open_clip_torch-2.29.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy, open_clip_torch\nSuccessfully installed ftfy-6.3.1 open_clip_torch-2.29.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import yaml\nimport os\nconfig_data = {\n    \"paths\": {\n        \"data_dir\": \"/kaggle/working/binclas\",\n        \"log_dir\": \"logs\",\n        \"cache_dir\": \"/kaggle/working/\",\n        \"checkpoint_dir\": \"/kaggle/working/\"\n    },\n    \"wandb\": {\n        \"using\": False,\n        \"api_key\": \"your_wandb_api_key\",\n        \"project\": \"project_name\",\n        \"run_name_template\": \"{hidden_dim}x{num_hidden_layers}_training\"\n    },\n    \"training\": {\n        \"batch_size\":4 ,\n        \"num_epochs\": 10,\n        \"accumulation_steps\": 2\n    },\n    \"model\": {\n        \"name\": \"ViT-L-14-quickgelu\", # Replace the Model with desired CLIP Model\n        \"pretrained\": \"openai\", # Corresponding Pretrained Dataset\n        \"clip_dim\": 768, # Don't forget to change this\n        \"hidden_dim\": [256],\n        \"dropout_rate\": [0.1],\n        \"num_hidden_layers\": [1],\n    },\n    \"optimizer\": {\n        \"clip_lr\": [1e-],\n        \"predictor_lr\": [5e-5],\n        \"weight_decay\": [0.001],\n        \"beta1\": [0.9],\n        \"beta2\": [0.999]\n    },\n    \"scheduler\": {\n        \"gamma\": 0.1,\n        \"milestones\": [4, 6, 10]\n    }\n}\n\noutput_dir = \"/kaggle/working/\"  # Replace with the desired directory\nfile_name = \"config.yml\"\n\n# Ensure the directory exists\nos.makedirs(output_dir, exist_ok=True)\n\n# Full file path\nfile_path = os.path.join(output_dir, file_name)\n\n# Write the YAML content to the file\nwith open(file_path, \"w\") as file:\n    yaml.dump(config_data, file, default_flow_style=False)\n\nprint(f\"YAML file saved to {file_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:23:29.842881Z","iopub.execute_input":"2024-12-08T22:23:29.843550Z","iopub.status.idle":"2024-12-08T22:23:29.857538Z","shell.execute_reply.started":"2024-12-08T22:23:29.843507Z","shell.execute_reply":"2024-12-08T22:23:29.856663Z"}},"outputs":[{"name":"stdout","text":"YAML file saved to /kaggle/working/config.yml\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# import os\n# import random\n# import shutil\n\n# # Define folder paths\n# scratches_folder = \"/kaggle/input/binnary-classification/scratches\"\n# no_scratches_folder = \"/kaggle/input/binnary-classification/no_scratches\"\n# test_images_folder = \"/kaggle/working/binclas\"\n# remaining_images_folder = \"/kaggle/working/testdata\"\n\n# # Create test_images and testdata folders if they don't exist\n# os.makedirs(test_images_folder, exist_ok=True)\n# os.makedirs(remaining_images_folder, exist_ok=True)\n\n# # Function to copy random files from a directory while preserving structure\n# def copy_random_files_with_structure(source_folder, destination_folder, subfolder_name, count):\n#     # Create subfolder in the destination folder\n#     destination_subfolder = os.path.join(destination_folder, subfolder_name)\n#     os.makedirs(destination_subfolder, exist_ok=True)\n    \n#     # Get all files in the source folder\n#     files = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n#     # Randomly select files\n#     selected_files = random.sample(files, min(count, len(files)))\n#     # Copy each selected file to the destination subfolder\n#     for file in selected_files:\n#         shutil.copy(os.path.join(source_folder, file), destination_subfolder)\n#     # Return remaining files\n#     remaining_files = set(files) - set(selected_files)\n#     return remaining_files\n\n# # Copy remaining files and preserve structure\n# def copy_remaining_files_with_structure(source_folder, destination_folder, remaining_files, subfolder_name):\n#     # Create subfolder in the destination folder\n#     destination_subfolder = os.path.join(destination_folder, subfolder_name)\n#     os.makedirs(destination_subfolder, exist_ok=True)\n    \n#     # Copy each remaining file to the destination subfolder\n#     for file in remaining_files:\n#         shutil.copy(os.path.join(source_folder, file), destination_subfolder)\n\n# # Copy 950 random images and maintain structure for `binclas`\n# remaining_scratches = copy_random_files_with_structure(scratches_folder, test_images_folder, \"scratches\", 950)\n# remaining_no_scratches = copy_random_files_with_structure(no_scratches_folder, test_images_folder, \"no_scratches\", 950)\n\n# # Copy remaining images and maintain structure for `testdata`\n# copy_remaining_files_with_structure(scratches_folder, remaining_images_folder, remaining_scratches, \"scratches\")\n# copy_remaining_files_with_structure(no_scratches_folder, remaining_images_folder, remaining_no_scratches, \"no_scratches\")\n\n# print(\"Files copied successfully with folder structure preserved for both `binclas` and `testdata`!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:10:46.555946Z","iopub.execute_input":"2024-12-08T22:10:46.556311Z","iopub.status.idle":"2024-12-08T22:11:29.687495Z","shell.execute_reply.started":"2024-12-08T22:10:46.556280Z","shell.execute_reply":"2024-12-08T22:11:29.686624Z"}},"outputs":[{"name":"stdout","text":"Files copied successfully with folder structure preserved for both `binclas` and `testdata`!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nimport json\nfrom datetime import datetime\nimport logging\nimport sys\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nimport open_clip\nfrom torch.optim.lr_scheduler import MultiStepLR\nimport yaml\nimport argparse\n\n##################\n# Configuration for Binary Classification\n##################\n\n# Single category/attribute mapping for binary classification\n# \"defect_scratch\" has 2 classes: no_scratch(0) and scratch(1)\nCATEGORY_MAPPING = {\n    \"defect\": {\n        \"scratch\": \"class\"  # just a placeholder key\n    }\n}\n\ndef load_config(config_path):\n    with open(config_path, 'r') as f:\n        return yaml.safe_load(f)\n\ndef setup_logging(config):\n    \"\"\"Set up logging configuration\"\"\"\n    os.makedirs(config['paths']['log_dir'], exist_ok=True)\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = os.path.join(config['paths']['log_dir'], f'vith14_binary_class_{timestamp}.log')\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s | %(levelname)s | %(message)s',\n        handlers=[\n            logging.FileHandler(log_file),\n            logging.StreamHandler(sys.stdout)\n        ]\n    )\n    return logging.getLogger(__name__)\n\n##################\n# Dataset\n##################\n\nclass BinaryClassificationDataset(Dataset):\n    def __init__(self, root_dir, clip_preprocess_train, clip_preprocess_val, train=True):\n        \"\"\"\n        root_dir structure:\n         root_dir/\n           scratches/\n             img_1.jpg\n             img_2.jpg\n           no_scratches/\n             img_a.jpg\n             img_b.jpg\n\n        We'll assign:\n        - scratch: label=1\n        - no_scratch: label=0\n        \"\"\"\n        self.root_dir = root_dir\n        self.classes = [\"no_scratches\", \"scratches\"]  # order defines labeling: no_scratches=0, scratches=1\n        self.filepaths = []\n        self.labels = []\n        self.train = train\n\n        for class_idx, class_name in enumerate(self.classes):\n            class_dir = os.path.join(root_dir, class_name)\n            for fname in os.listdir(class_dir):\n                if fname.lower().endswith(('.jpg', '.png', '.jpeg')):\n                    self.filepaths.append(os.path.join(class_dir, fname))\n                    self.labels.append(class_idx)\n        \n        self.clip_preprocess_train = clip_preprocess_train\n        self.clip_preprocess_val = clip_preprocess_val\n\n    def __len__(self):\n        return len(self.filepaths)\n\n    def __getitem__(self, idx):\n        image_path = self.filepaths[idx]\n        label = self.labels[idx]\n        category = \"defect\"  # a single category for all samples\n\n        image = Image.open(image_path).convert('RGB')\n        if self.train:\n            image = self.clip_preprocess_train(image)\n        else:\n            image = self.clip_preprocess_val(image)\n\n        # We have a single attribute \"scratch\"\n        # Our target dict: {\"defect_scratch\": label}\n        targets = {\"defect_scratch\": label}\n\n        return image, category, targets\n\n##################\n# Collate Function\n##################\ndef custom_collate_fn(batch):\n    images = torch.stack([item[0] for item in batch])\n    categories = [item[1] for item in batch]\n\n    # Single attribute target\n    targets = {\"defect_scratch\": torch.tensor([item[2][\"defect_scratch\"] for item in batch], dtype=torch.long)}\n\n    return images, categories, targets\n\n##################\n# Model\n##################\n\nclass CategoryAwareAttributePredictor(nn.Module):\n    def __init__(self, clip_dim=768, category_attributes=None, attribute_dims=None, hidden_dim=512, dropout_rate=0.2, num_hidden_layers=1):\n        super(CategoryAwareAttributePredictor, self).__init__()\n        \n        self.category_attributes = category_attributes\n        self.attribute_predictors = nn.ModuleDict()\n        \n        for category, attributes in category_attributes.items():\n            for attr_name in attributes.keys():\n                key = f\"{category}_{attr_name}\"\n                if key in attribute_dims:\n                    layers = []\n                    # Input layer\n                    layers.append(nn.Linear(clip_dim, hidden_dim))\n                    layers.append(nn.LayerNorm(hidden_dim))\n                    layers.append(nn.ReLU())\n                    layers.append(nn.Dropout(dropout_rate))\n                    \n                    # Additional hidden layers\n                    current_dim = hidden_dim\n                    for _ in range(num_hidden_layers - 1):\n                        layers.append(nn.Linear(current_dim, current_dim // 2))\n                        layers.append(nn.LayerNorm(current_dim // 2))\n                        layers.append(nn.ReLU())\n                        layers.append(nn.Dropout(dropout_rate))\n                        current_dim = current_dim // 2\n\n                    # Output layer\n                    layers.append(nn.Linear(current_dim, attribute_dims[key]))\n                    \n                    self.attribute_predictors[key] = nn.Sequential(*layers)\n    \n    def forward(self, clip_features, category):\n        results = {}\n        category_attrs = self.category_attributes[category]\n        clip_features = clip_features.float()\n        \n        for attr_name in category_attrs.keys():\n            key = f\"{category}_{attr_name}\"\n            if key in self.attribute_predictors:\n                results[key] = self.attribute_predictors[key](clip_features)\n        \n        return results\n\n##################\n# Training Function\n##################\n\ndef train_model(\n        clip_model, \n        model, \n        train_dataset,\n        train_loader, \n        device,  \n        clip_lr,\n        predictor_lr, \n        weight_decay, \n        beta1, \n        beta2, \n        hidden_dim, \n        dropout_rate, \n        num_hidden_layers, \n        milestones, \n        gamma, \n        logger, \n        checkpoint_dir,\n        num_epochs=10, \n        accumulation_steps=2\n    ):\n\n    logger.info(\"Starting training...\")\n    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n\n    optimizer = optim.AdamW(\n        [\n            {'params': clip_model.parameters(), 'lr': clip_lr},\n            {'params': model.parameters(), 'lr': predictor_lr, 'weight_decay': weight_decay}\n        ], \n        betas=(beta1, beta2)\n    )\n    scaler = torch.cuda.amp.GradScaler()\n\n    scheduler = MultiStepLR(optimizer=optimizer, milestones=milestones, gamma=gamma)\n    metrics_history = {'train_loss': [], 'train_acc': []}\n    \n    for epoch in range(num_epochs):\n        print(\"here\")\n        logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n        model.train()\n        clip_model.train()\n\n        train_loss = 0.0\n        correct = 0\n        total = 0\n        num_batches = 0\n\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n        # print(\"abbbtooo?\")\n        optimizer.zero_grad()\n\n        for batch_idx, (images, categories, targets) in enumerate(pbar):\n            # print(\"or yhaan?\")\n            images = images.to(device)\n            batch_size = images.size(0)\n\n            with torch.cuda.amp.autocast():\n                image_features = clip_model.encode_image(images)\n                # print(\"idhar kyaaa?\")\n                # For each sample we get predictions\n                # We have only one attribute: defect_scratch\n                predictions = model(image_features, categories[0])  # all belong to \"defect\"\n                pred_logits = predictions[\"defect_scratch\"]  # shape [batch_size, 2]\n\n                target_vals = targets[\"defect_scratch\"].to(device)\n                loss = criterion(pred_logits, target_vals)\n                loss = loss / accumulation_steps\n                # print(loss)\n\n            scaler.scale(loss).backward()\n\n            if (batch_idx + 1) % accumulation_steps == 0:\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n\n            # Metrics\n            train_loss += loss.item() * accumulation_steps\n            num_batches += 1\n\n            _, predicted = torch.max(pred_logits, 1)\n            correct += (predicted == target_vals).sum().item()\n            total += batch_size\n\n            pbar.set_postfix(loss=(train_loss/num_batches), acc=correct/total)\n\n        scheduler.step()\n\n        avg_loss = train_loss / num_batches\n        accuracy = correct / total\n        metrics_history['train_loss'].append(avg_loss)\n        metrics_history['train_acc'].append(accuracy)\n\n        logger.info(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}\")\n\n        # Save a checkpoint each epoch\n        os.makedirs(checkpoint_dir, exist_ok=True)\n        save_path = os.path.join(checkpoint_dir, f\"binary_checkpoint_epoch{epoch+1}.pth\")\n        torch.save({\n            'model_state_dict': model.state_dict(),\n            'clip_model_state_dict': clip_model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'metrics': metrics_history\n        }, save_path)\n\n    logger.info(\"Training completed.\")\n    return model, clip_model, metrics_history\n\n##################\n# Main\n##################\ndef main():\n    # Instead of using argparse, just directly use `config_data`\n    config = config_data\n\n    # Set up logging\n    logger = setup_logging(config)\n    print(\"helooooooooooooo\")\n    logger.info(\"Starting training with provided config\")\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"imhere22222222\")\n    # Create CLIP model\n    clip_model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n        config['model']['name'],\n        pretrained=config['model']['pretrained'],\n        device=device\n    )\n    print(\"aayakyaaa\")\n    clip_model = clip_model.float()\n\n    # Prepare dataset\n    train_dir = os.path.join(config['paths']['data_dir'])\n    train_dataset = BinaryClassificationDataset(\n        root_dir=train_dir,\n        clip_preprocess_train=preprocess_train,\n        clip_preprocess_val=preprocess_val,\n        train=True\n    )\n    print(\"aayakyaa222\")\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['training']['batch_size'],\n        shuffle=True,\n        num_workers=8,\n        pin_memory=True,\n        collate_fn=custom_collate_fn\n    )\n    print(\"haan bhai\")\n    # Define attribute_dims for binary classification\n    attribute_dims = {\"defect_scratch\": 2}\n\n    model = CategoryAwareAttributePredictor(\n        clip_dim=config['model']['clip_dim'],\n        category_attributes=CATEGORY_MAPPING,\n        attribute_dims=attribute_dims,\n        hidden_dim=config['model']['hidden_dim'][0],\n        dropout_rate=config['model']['dropout_rate'][0],\n        num_hidden_layers=config['model']['num_hidden_layers'][0]\n    ).to(device)\n    print(\"abtobol\")\n    model, clip_model, metrics = train_model(\n        clip_model=clip_model,\n        model=model,\n        train_dataset=train_dataset,\n        train_loader=train_loader,\n        device=device,\n        clip_lr=config['optimizer']['clip_lr'][0],\n        predictor_lr=config['optimizer']['predictor_lr'][0],\n        weight_decay=config['optimizer']['weight_decay'][0],\n        beta1=config['optimizer']['beta1'][0],\n        beta2=config['optimizer']['beta2'][0],\n        hidden_dim=config['model']['hidden_dim'][0],\n        dropout_rate=config['model']['dropout_rate'][0],\n        num_hidden_layers=config['model']['num_hidden_layers'][0],\n        gamma=config['scheduler']['gamma'],\n        milestones=config['scheduler']['milestones'],\n        logger=logger,\n        checkpoint_dir=config['paths']['checkpoint_dir'],\n        num_epochs=config['training']['num_epochs'],\n        accumulation_steps=config['training']['accumulation_steps']\n    )\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:23:32.197482Z","iopub.execute_input":"2024-12-08T22:23:32.197851Z","iopub.status.idle":"2024-12-08T22:23:32.229639Z","shell.execute_reply.started":"2024-12-08T22:23:32.197817Z","shell.execute_reply":"2024-12-08T22:23:32.228971Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:23:32.751294Z","iopub.execute_input":"2024-12-08T22:23:32.752124Z","iopub.status.idle":"2024-12-08T22:25:51.677423Z","shell.execute_reply.started":"2024-12-08T22:23:32.752085Z","shell.execute_reply":"2024-12-08T22:25:51.676129Z"}},"outputs":[{"name":"stdout","text":"helooooooooooooo\nimhere22222222\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/4146927518.py:203: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"aayakyaaa\naayakyaa222\nhaan bhai\nabtobol\nhere\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:   0%|          | 0/475 [00:00<?, ?it/s]/tmp/ipykernel_23/4146927518.py:228: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEpoch 1: 100%|██████████| 475/475 [01:52<00:00,  4.21it/s, acc=0.879, loss=0.285]\n","output_type":"stream"},{"name":"stdout","text":"here\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  10%|▉         | 47/475 [00:11<01:47,  4.00it/s, acc=0.952, loss=0.109] \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[18], line 334\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    325\u001b[0m model \u001b[38;5;241m=\u001b[39m CategoryAwareAttributePredictor(\n\u001b[1;32m    326\u001b[0m     clip_dim\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip_dim\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    327\u001b[0m     category_attributes\u001b[38;5;241m=\u001b[39mCATEGORY_MAPPING,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m     num_hidden_layers\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_hidden_layers\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    332\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabtobol\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 334\u001b[0m model, clip_model, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclip_lr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictor_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictor_lr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeta1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeta2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropout_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_hidden_layers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscheduler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgamma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmilestones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscheduler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmilestones\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpaths\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoint_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccumulation_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[18], line 241\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(clip_model, model, train_dataset, train_loader, device, clip_lr, predictor_lr, weight_decay, beta1, beta2, hidden_dim, dropout_rate, num_hidden_layers, milestones, gamma, logger, checkpoint_dir, num_epochs, accumulation_steps)\u001b[0m\n\u001b[1;32m    238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m accumulation_steps\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    244\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom PIL import Image\nimport open_clip\nimport yaml\nimport os\n\n##################\n# CATEGORY_MAPPING\n##################\nCATEGORY_MAPPING = {\n    \"defect\": {\n        \"scratch\": \"class\"\n    }\n}\n\n##################\n# CategoryAwareAttributePredictor\n##################\nclass CategoryAwareAttributePredictor(nn.Module):\n    def __init__(self, clip_dim=768, category_attributes=None, attribute_dims=None, hidden_dim=512, dropout_rate=0.2, num_hidden_layers=1):\n        super(CategoryAwareAttributePredictor, self).__init__()\n        \n        self.category_attributes = category_attributes\n        self.attribute_predictors = nn.ModuleDict()\n        \n        for category, attributes in category_attributes.items():\n            for attr_name in attributes.keys():\n                key = f\"{category}_{attr_name}\"\n                if key in attribute_dims:\n                    layers = []\n                    # Input layer\n                    layers.append(nn.Linear(clip_dim, hidden_dim))\n                    layers.append(nn.LayerNorm(hidden_dim))\n                    layers.append(nn.ReLU())\n                    layers.append(nn.Dropout(dropout_rate))\n                    \n                    # Additional hidden layers\n                    current_dim = hidden_dim\n                    for _ in range(num_hidden_layers - 1):\n                        layers.append(nn.Linear(current_dim, current_dim // 2))\n                        layers.append(nn.LayerNorm(current_dim // 2))\n                        layers.append(nn.ReLU())\n                        layers.append(nn.Dropout(dropout_rate))\n                        current_dim = current_dim // 2\n\n                    # Output layer\n                    layers.append(nn.Linear(current_dim, attribute_dims[key]))\n                    \n                    self.attribute_predictors[key] = nn.Sequential(*layers)\n    \n    def forward(self, clip_features, category):\n        results = {}\n        category_attrs = self.category_attributes[category]\n        clip_features = clip_features.float()\n        \n        for attr_name in category_attrs.keys():\n            key = f\"{category}_{attr_name}\"\n            if key in self.attribute_predictors:\n                results[key] = self.attribute_predictors[key](clip_features)\n        \n        return results\n\n##################\n# Helper Functions\n##################\n\ndef load_config(config_path):\n    with open(config_path, 'r') as f:\n        return yaml.safe_load(f)\n\ndef load_models(config, checkpoint_path, device):\n    # Create CLIP model and transforms\n    clip_model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n        config['model']['name'],\n        pretrained=config['model']['pretrained'],\n        device=device\n    )\n    clip_model = clip_model.float()\n    \n    # Define attribute_dims (binary classification: 2 classes)\n    attribute_dims = {\"defect_scratch\": 2}\n    \n    model = CategoryAwareAttributePredictor(\n        clip_dim=config['model']['clip_dim'],\n        category_attributes=CATEGORY_MAPPING,\n        attribute_dims=attribute_dims,\n        hidden_dim=config['model']['hidden_dim'][0],\n        dropout_rate=config['model']['dropout_rate'][0],\n        num_hidden_layers=config['model']['num_hidden_layers'][0]\n    ).to(device)\n\n    # Load checkpoint\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    clip_model.load_state_dict(checkpoint['clip_model_state_dict'])\n\n    model.eval()\n    clip_model.eval()\n    \n    return clip_model, model, preprocess_val\n\ndef infer_image(clip_model, model, preprocess, image_path, device):\n    image = Image.open(image_path).convert('RGB')\n    image_tensor = preprocess(image).unsqueeze(0).to(device)\n\n    category = \"defect\"  # known from training\n    \n    with torch.no_grad():\n        image_features = clip_model.encode_image(image_tensor)\n        predictions = model(image_features, category)\n        logits = predictions[\"defect_scratch\"]  # shape [1, 2]\n        probs = F.softmax(logits, dim=1)\n        pred_class = torch.argmax(probs, dim=1).item()\n\n        class_names = [\"no_scratches\", \"scratches\"]\n        pred_label = class_names[pred_class]\n        \n        return pred_label, probs.cpu().numpy()\n\n##################\n# Example usage (adjust paths as needed)\n##################\nif __name__ == \"__main__\":\n    # Path to config and checkpoint\n    config_path = \"/kaggle/working/config.yml\"\n    checkpoint_path = \"/kaggle/working/binary_checkpoint_epoch5.pth\"\n    test_image_path = \"/kaggle/working/testdata/scratches/Code01947.png\"\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    config = load_config(config_path)\n\n    # Load models\n    clip_model, model, preprocess_val = load_models(config, checkpoint_path, device)\n\n    # Run inference on a test image\n    pred_label, probs = infer_image(clip_model, model, preprocess_val, test_image_path, device)\n    print(f\"Predicted label: {pred_label}, Probabilities: {probs}\")\n\n# if __name__ == \"__main__\":\n#     # Example usage:\n#     # Provide the path to your config and a checkpoint file\n#     config_path = \"/kaggle/working/config.yml\"\n#     checkpoint_path = \"/kaggle/working/binary_checkpoint_epoch7.pth\"\n#     test_image_path = \"/kaggle/input/binnary-classification/scratches/03_08_2024_17_12_41.304965_classifier_input.png\"\n\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     config = load_config(config_path)\n#     clip_model, model, preprocess_val = load_models(config, checkpoint_path, device)\n\n#     pred_label, probs = infer_image(clip_model, model, preprocess_val, test_image_path, device)\n#     print(f\"Predicted label: {pred_label}, Probabilities: {probs}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:16:39.612226Z","iopub.execute_input":"2024-12-08T22:16:39.612821Z","iopub.status.idle":"2024-12-08T22:16:42.663376Z","shell.execute_reply.started":"2024-12-08T22:16:39.612784Z","shell.execute_reply":"2024-12-08T22:16:42.662393Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/1559926611.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Predicted label: scratches, Probabilities: [[0.00655092 0.99344915]]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom sklearn.metrics import precision_recall_fscore_support\n\n# Function to perform inference on a single image\ndef infer_image(clip_model, model, preprocess, image_path, device):\n    image = Image.open(image_path).convert('RGB')\n    image_tensor = preprocess(image).unsqueeze(0).to(device)\n\n    category = \"defect\"  # Known from training\n    \n    with torch.no_grad():\n        image_features = clip_model.encode_image(image_tensor)\n        predictions = model(image_features, category)\n        logits = predictions[\"defect_scratch\"]  # Shape [1, 2]\n        probs = F.softmax(logits, dim=1)\n        pred_class = torch.argmax(probs, dim=1).item()\n\n        class_names = [\"no_scratches\", \"scratches\"]\n        pred_label = class_names[pred_class]\n        \n        return pred_label, probs.cpu().numpy()\n\n# Function to calculate metrics\ndef evaluate_model(clip_model, model, preprocess, test_folder, device, num_samples=150):\n    class_names = [\"no_scratches\", \"scratches\"]\n    \n    # Collect all test image paths and labels\n    test_images = []\n    labels = []\n    for class_name in class_names:\n        class_folder = os.path.join(test_folder, class_name)\n        for file_name in os.listdir(class_folder):\n            if file_name.endswith(('.png', '.jpg', '.jpeg')):\n                test_images.append(os.path.join(class_folder, file_name))\n                labels.append(class_name)\n    \n    # Randomly select a subset of images for evaluation\n    selected_indices = random.sample(range(len(test_images)), min(num_samples, len(test_images)))\n    selected_images = [test_images[i] for i in selected_indices]\n    selected_labels = [labels[i] for i in selected_indices]\n    \n    # Perform inference on the selected images\n    preds = []\n    for image_path in selected_images:\n        pred_label, _ = infer_image(clip_model, model, preprocess, image_path, device)\n        preds.append(pred_label)\n    \n    # Convert class names to binary labels\n    label_map = {name: idx for idx, name in enumerate(class_names)}\n    y_true = [label_map[label] for label in selected_labels]\n    y_pred = [label_map[pred] for pred in preds]\n    \n    # Calculate precision, recall, and F1 score\n    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n    \n    return precision, recall, f1\n\n# Main script\nif __name__ == \"__main__\":\n    # Path to config and checkpoint\n    config_path = \"/kaggle/working/config.yml\"\n    checkpoint_path = \"/kaggle/working/binary_checkpoint_epoch5.pth\"\n    test_folder = \"/kaggle/working/testdata\"\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    config = load_config(config_path)\n\n    # Load models\n    clip_model, model, preprocess_val = load_models(config, checkpoint_path, device)\n\n    # Evaluate the model\n    precision, recall, f1 = evaluate_model(clip_model, model, preprocess_val, test_folder, device, num_samples=100)\n    \n    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:20:57.829910Z","iopub.execute_input":"2024-12-08T22:20:57.830286Z","iopub.status.idle":"2024-12-08T22:21:01.579144Z","shell.execute_reply.started":"2024-12-08T22:20:57.830255Z","shell.execute_reply":"2024-12-08T22:21:01.578217Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/open_clip/factory.py:372: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n  warnings.warn(\n/tmp/ipykernel_23/1559926611.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Precision: 0.2000, Recall: 1.0000, F1 Score: 0.3333\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import open_clip\n\navailable_models = open_clip.list_pretrained()\nfor model_name, pretrained_name in available_models:\n    print(f\"Model: {model_name}, Pretrained: {pretrained_name}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:21:38.248000Z","iopub.execute_input":"2024-12-08T22:21:38.248359Z","iopub.status.idle":"2024-12-08T22:21:38.255113Z","shell.execute_reply.started":"2024-12-08T22:21:38.248319Z","shell.execute_reply":"2024-12-08T22:21:38.254025Z"}},"outputs":[{"name":"stdout","text":"Model: RN50, Pretrained: openai\nModel: RN50, Pretrained: yfcc15m\nModel: RN50, Pretrained: cc12m\nModel: RN101, Pretrained: openai\nModel: RN101, Pretrained: yfcc15m\nModel: RN50x4, Pretrained: openai\nModel: RN50x16, Pretrained: openai\nModel: RN50x64, Pretrained: openai\nModel: ViT-B-32, Pretrained: openai\nModel: ViT-B-32, Pretrained: laion400m_e31\nModel: ViT-B-32, Pretrained: laion400m_e32\nModel: ViT-B-32, Pretrained: laion2b_e16\nModel: ViT-B-32, Pretrained: laion2b_s34b_b79k\nModel: ViT-B-32, Pretrained: datacomp_xl_s13b_b90k\nModel: ViT-B-32, Pretrained: datacomp_m_s128m_b4k\nModel: ViT-B-32, Pretrained: commonpool_m_clip_s128m_b4k\nModel: ViT-B-32, Pretrained: commonpool_m_laion_s128m_b4k\nModel: ViT-B-32, Pretrained: commonpool_m_image_s128m_b4k\nModel: ViT-B-32, Pretrained: commonpool_m_text_s128m_b4k\nModel: ViT-B-32, Pretrained: commonpool_m_basic_s128m_b4k\nModel: ViT-B-32, Pretrained: commonpool_m_s128m_b4k\nModel: ViT-B-32, Pretrained: datacomp_s_s13m_b4k\nModel: ViT-B-32, Pretrained: commonpool_s_clip_s13m_b4k\nModel: ViT-B-32, Pretrained: commonpool_s_laion_s13m_b4k\nModel: ViT-B-32, Pretrained: commonpool_s_image_s13m_b4k\nModel: ViT-B-32, Pretrained: commonpool_s_text_s13m_b4k\nModel: ViT-B-32, Pretrained: commonpool_s_basic_s13m_b4k\nModel: ViT-B-32, Pretrained: commonpool_s_s13m_b4k\nModel: ViT-B-32, Pretrained: metaclip_400m\nModel: ViT-B-32, Pretrained: metaclip_fullcc\nModel: ViT-B-32-256, Pretrained: datacomp_s34b_b86k\nModel: ViT-B-16, Pretrained: openai\nModel: ViT-B-16, Pretrained: laion400m_e31\nModel: ViT-B-16, Pretrained: laion400m_e32\nModel: ViT-B-16, Pretrained: laion2b_s34b_b88k\nModel: ViT-B-16, Pretrained: datacomp_xl_s13b_b90k\nModel: ViT-B-16, Pretrained: datacomp_l_s1b_b8k\nModel: ViT-B-16, Pretrained: commonpool_l_clip_s1b_b8k\nModel: ViT-B-16, Pretrained: commonpool_l_laion_s1b_b8k\nModel: ViT-B-16, Pretrained: commonpool_l_image_s1b_b8k\nModel: ViT-B-16, Pretrained: commonpool_l_text_s1b_b8k\nModel: ViT-B-16, Pretrained: commonpool_l_basic_s1b_b8k\nModel: ViT-B-16, Pretrained: commonpool_l_s1b_b8k\nModel: ViT-B-16, Pretrained: dfn2b\nModel: ViT-B-16, Pretrained: metaclip_400m\nModel: ViT-B-16, Pretrained: metaclip_fullcc\nModel: ViT-B-16-plus-240, Pretrained: laion400m_e31\nModel: ViT-B-16-plus-240, Pretrained: laion400m_e32\nModel: ViT-L-14, Pretrained: openai\nModel: ViT-L-14, Pretrained: laion400m_e31\nModel: ViT-L-14, Pretrained: laion400m_e32\nModel: ViT-L-14, Pretrained: laion2b_s32b_b82k\nModel: ViT-L-14, Pretrained: datacomp_xl_s13b_b90k\nModel: ViT-L-14, Pretrained: commonpool_xl_clip_s13b_b90k\nModel: ViT-L-14, Pretrained: commonpool_xl_laion_s13b_b90k\nModel: ViT-L-14, Pretrained: commonpool_xl_s13b_b90k\nModel: ViT-L-14, Pretrained: metaclip_400m\nModel: ViT-L-14, Pretrained: metaclip_fullcc\nModel: ViT-L-14, Pretrained: dfn2b\nModel: ViT-L-14-336, Pretrained: openai\nModel: ViT-H-14, Pretrained: laion2b_s32b_b79k\nModel: ViT-H-14, Pretrained: metaclip_fullcc\nModel: ViT-H-14, Pretrained: dfn5b\nModel: ViT-H-14-378, Pretrained: dfn5b\nModel: ViT-g-14, Pretrained: laion2b_s12b_b42k\nModel: ViT-g-14, Pretrained: laion2b_s34b_b88k\nModel: ViT-bigG-14, Pretrained: laion2b_s39b_b160k\nModel: ViT-bigG-14, Pretrained: metaclip_fullcc\nModel: roberta-ViT-B-32, Pretrained: laion2b_s12b_b32k\nModel: xlm-roberta-base-ViT-B-32, Pretrained: laion5b_s13b_b90k\nModel: xlm-roberta-large-ViT-H-14, Pretrained: frozen_laion5b_s13b_b90k\nModel: convnext_base, Pretrained: laion400m_s13b_b51k\nModel: convnext_base_w, Pretrained: laion2b_s13b_b82k\nModel: convnext_base_w, Pretrained: laion2b_s13b_b82k_augreg\nModel: convnext_base_w, Pretrained: laion_aesthetic_s13b_b82k\nModel: convnext_base_w_320, Pretrained: laion_aesthetic_s13b_b82k\nModel: convnext_base_w_320, Pretrained: laion_aesthetic_s13b_b82k_augreg\nModel: convnext_large_d, Pretrained: laion2b_s26b_b102k_augreg\nModel: convnext_large_d_320, Pretrained: laion2b_s29b_b131k_ft\nModel: convnext_large_d_320, Pretrained: laion2b_s29b_b131k_ft_soup\nModel: convnext_xxlarge, Pretrained: laion2b_s34b_b82k_augreg\nModel: convnext_xxlarge, Pretrained: laion2b_s34b_b82k_augreg_rewind\nModel: convnext_xxlarge, Pretrained: laion2b_s34b_b82k_augreg_soup\nModel: coca_ViT-B-32, Pretrained: laion2b_s13b_b90k\nModel: coca_ViT-B-32, Pretrained: mscoco_finetuned_laion2b_s13b_b90k\nModel: coca_ViT-L-14, Pretrained: laion2b_s13b_b90k\nModel: coca_ViT-L-14, Pretrained: mscoco_finetuned_laion2b_s13b_b90k\nModel: EVA01-g-14, Pretrained: laion400m_s11b_b41k\nModel: EVA01-g-14-plus, Pretrained: merged2b_s11b_b114k\nModel: EVA02-B-16, Pretrained: merged2b_s8b_b131k\nModel: EVA02-L-14, Pretrained: merged2b_s4b_b131k\nModel: EVA02-L-14-336, Pretrained: merged2b_s6b_b61k\nModel: EVA02-E-14, Pretrained: laion2b_s4b_b115k\nModel: EVA02-E-14-plus, Pretrained: laion2b_s9b_b144k\nModel: ViT-B-16-SigLIP, Pretrained: webli\nModel: ViT-B-16-SigLIP-256, Pretrained: webli\nModel: ViT-B-16-SigLIP-i18n-256, Pretrained: webli\nModel: ViT-B-16-SigLIP-384, Pretrained: webli\nModel: ViT-B-16-SigLIP-512, Pretrained: webli\nModel: ViT-L-16-SigLIP-256, Pretrained: webli\nModel: ViT-L-16-SigLIP-384, Pretrained: webli\nModel: ViT-SO400M-14-SigLIP, Pretrained: webli\nModel: ViT-SO400M-16-SigLIP-i18n-256, Pretrained: webli\nModel: ViT-SO400M-14-SigLIP-378, Pretrained: webli\nModel: ViT-SO400M-14-SigLIP-384, Pretrained: webli\nModel: ViT-L-14-CLIPA, Pretrained: datacomp1b\nModel: ViT-L-14-CLIPA-336, Pretrained: datacomp1b\nModel: ViT-H-14-CLIPA, Pretrained: datacomp1b\nModel: ViT-H-14-CLIPA-336, Pretrained: laion2b\nModel: ViT-H-14-CLIPA-336, Pretrained: datacomp1b\nModel: ViT-bigG-14-CLIPA, Pretrained: datacomp1b\nModel: ViT-bigG-14-CLIPA-336, Pretrained: datacomp1b\nModel: nllb-clip-base, Pretrained: v1\nModel: nllb-clip-large, Pretrained: v1\nModel: nllb-clip-base-siglip, Pretrained: v1\nModel: nllb-clip-base-siglip, Pretrained: mrl\nModel: nllb-clip-large-siglip, Pretrained: v1\nModel: nllb-clip-large-siglip, Pretrained: mrl\nModel: MobileCLIP-S1, Pretrained: datacompdr\nModel: MobileCLIP-S2, Pretrained: datacompdr\nModel: MobileCLIP-B, Pretrained: datacompdr\nModel: MobileCLIP-B, Pretrained: datacompdr_lt\nModel: ViTamin-S, Pretrained: datacomp1b\nModel: ViTamin-S-LTT, Pretrained: datacomp1b\nModel: ViTamin-B, Pretrained: datacomp1b\nModel: ViTamin-B-LTT, Pretrained: datacomp1b\nModel: ViTamin-L, Pretrained: datacomp1b\nModel: ViTamin-L-256, Pretrained: datacomp1b\nModel: ViTamin-L-336, Pretrained: datacomp1b\nModel: ViTamin-L-384, Pretrained: datacomp1b\nModel: ViTamin-L2, Pretrained: datacomp1b\nModel: ViTamin-L2-256, Pretrained: datacomp1b\nModel: ViTamin-L2-336, Pretrained: datacomp1b\nModel: ViTamin-L2-384, Pretrained: datacomp1b\nModel: ViTamin-XL-256, Pretrained: datacomp1b\nModel: ViTamin-XL-336, Pretrained: datacomp1b\nModel: ViTamin-XL-384, Pretrained: datacomp1b\nModel: RN50-quickgelu, Pretrained: openai\nModel: RN50-quickgelu, Pretrained: yfcc15m\nModel: RN50-quickgelu, Pretrained: cc12m\nModel: RN101-quickgelu, Pretrained: openai\nModel: RN101-quickgelu, Pretrained: yfcc15m\nModel: RN50x4-quickgelu, Pretrained: openai\nModel: RN50x16-quickgelu, Pretrained: openai\nModel: RN50x64-quickgelu, Pretrained: openai\nModel: ViT-B-32-quickgelu, Pretrained: openai\nModel: ViT-B-32-quickgelu, Pretrained: laion400m_e31\nModel: ViT-B-32-quickgelu, Pretrained: laion400m_e32\nModel: ViT-B-32-quickgelu, Pretrained: metaclip_400m\nModel: ViT-B-32-quickgelu, Pretrained: metaclip_fullcc\nModel: ViT-B-16-quickgelu, Pretrained: openai\nModel: ViT-B-16-quickgelu, Pretrained: dfn2b\nModel: ViT-B-16-quickgelu, Pretrained: metaclip_400m\nModel: ViT-B-16-quickgelu, Pretrained: metaclip_fullcc\nModel: ViT-L-14-quickgelu, Pretrained: openai\nModel: ViT-L-14-quickgelu, Pretrained: metaclip_400m\nModel: ViT-L-14-quickgelu, Pretrained: metaclip_fullcc\nModel: ViT-L-14-quickgelu, Pretrained: dfn2b\nModel: ViT-L-14-336-quickgelu, Pretrained: openai\nModel: ViT-H-14-quickgelu, Pretrained: metaclip_fullcc\nModel: ViT-H-14-quickgelu, Pretrained: dfn5b\nModel: ViT-H-14-378-quickgelu, Pretrained: dfn5b\nModel: ViT-bigG-14-quickgelu, Pretrained: metaclip_fullcc\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
