{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10138538,"sourceType":"datasetVersion","datasetId":6257190}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install open_clip_torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T17:27:58.292104Z","iopub.execute_input":"2024-12-10T17:27:58.292500Z","iopub.status.idle":"2024-12-10T17:28:08.301017Z","shell.execute_reply.started":"2024-12-10T17:27:58.292463Z","shell.execute_reply":"2024-12-10T17:28:08.300133Z"}},"outputs":[{"name":"stdout","text":"Collecting open_clip_torch\n  Downloading open_clip_torch-2.29.0-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.19.0)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2024.5.15)\nCollecting ftfy (from open_clip_torch)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (4.66.4)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.26.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.4.5)\nRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (1.0.11)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (2024.6.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy->open_clip_torch) (0.2.13)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (2.32.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->open_clip_torch) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->open_clip_torch) (10.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->open_clip_torch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\nDownloading open_clip_torch-2.29.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy, open_clip_torch\nSuccessfully installed ftfy-6.3.1 open_clip_torch-2.29.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import yaml\nimport os\nconfig_data = {\n    \"paths\": {\n        \"data_dir\": \"/kaggle/working/binclas\",\n        \"log_dir\": \"logs\",\n        \"cache_dir\": \"/kaggle/working/\",\n        \"checkpoint_dir\": \"/kaggle/working/\"\n    },\n    \"wandb\": {\n        \"using\": False,\n        \"api_key\": \"your_wandb_api_key\",\n        \"project\": \"project_name\",\n        \"run_name_template\": \"{hidden_dim}x{num_hidden_layers}_training\"\n    },\n    # Model: convnext_base_w, Pretrained: laion2b_s13b_b82k\n\n    \"training\": {\n        \"batch_size\":4 ,\n        \"num_epochs\": 10,\n        \"accumulation_steps\": 2\n    },\n    \"model\": {\n        \"name\": \"convnext_base_w\", # Replace the Model with desired CLIP Model\n        \"pretrained\": \"laion2b_s13b_b82k\", # Corresponding Pretrained Dataset\n        \"clip_dim\": 640, # Don't forget to change this\n        \"hidden_dim\": [256],\n        \"dropout_rate\": [0.1],\n        \"num_hidden_layers\": [1],\n    },\n    \"optimizer\": {\n        \"clip_lr\": [1e-5],\n        \"predictor_lr\": [5e-5],\n        \"weight_decay\": [0.001],\n        \"beta1\": [0.9],\n        \"beta2\": [0.999]\n    },\n    \"scheduler\": {\n        \"gamma\": 0.1,\n        \"milestones\": [4, 6, 10]\n    }\n}\n\noutput_dir = \"/kaggle/working/\"  # Replace with the desired directory\nfile_name = \"config.yml\"\n\n# Ensure the directory exists\nos.makedirs(output_dir, exist_ok=True)\n\n# Full file path\nfile_path = os.path.join(output_dir, file_name)\n\n# Write the YAML content to the file\nwith open(file_path, \"w\") as file:\n    yaml.dump(config_data, file, default_flow_style=False)\n\nprint(f\"YAML file saved to {file_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T18:03:15.436112Z","iopub.execute_input":"2024-12-10T18:03:15.436547Z","iopub.status.idle":"2024-12-10T18:03:15.448625Z","shell.execute_reply.started":"2024-12-10T18:03:15.436516Z","shell.execute_reply":"2024-12-10T18:03:15.447665Z"}},"outputs":[{"name":"stdout","text":"YAML file saved to /kaggle/working/config.yml\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import os\nimport random\nimport shutil\n\n# Define folder paths\nscratches_folder = \"/kaggle/input/binnary-classification/scratches\"\nno_scratches_folder = \"/kaggle/input/binnary-classification/no_scratches\"\ntest_images_folder = \"/kaggle/working/binclas\"\nremaining_images_folder = \"/kaggle/working/testdata\"\n\n# Create test_images and testdata folders if they don't exist\nos.makedirs(test_images_folder, exist_ok=True)\nos.makedirs(remaining_images_folder, exist_ok=True)\n\n# Function to copy random files from a directory while preserving structure\ndef copy_random_files_with_structure(source_folder, destination_folder, subfolder_name, percentage):\n    # Create subfolder in the destination folder\n    destination_subfolder = os.path.join(destination_folder, subfolder_name)\n    os.makedirs(destination_subfolder, exist_ok=True)\n    \n    # Get all files in the source folder\n    files = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n    # Calculate the number of files to copy\n    count = max(1, int(len(files) * percentage / 100))  # Ensure at least one file is selected if percentage > 0\n    # Randomly select files\n    selected_files = random.sample(files, min(count, len(files)))\n    # Copy each selected file to the destination subfolder\n    for file in selected_files:\n        shutil.copy(os.path.join(source_folder, file), destination_subfolder)\n    # Return remaining files\n    remaining_files = set(files) - set(selected_files)\n    return remaining_files\n\n# Copy remaining files and preserve structure\ndef copy_remaining_files_with_structure(source_folder, destination_folder, remaining_files, subfolder_name):\n    # Create subfolder in the destination folder\n    destination_subfolder = os.path.join(destination_folder, subfolder_name)\n    os.makedirs(destination_subfolder, exist_ok=True)\n    \n    # Copy each remaining file to the destination subfolder\n    for file in remaining_files:\n        shutil.copy(os.path.join(source_folder, file), destination_subfolder)\n\n# Copy 10% of images and maintain structure for `binclas`\nremaining_scratches = copy_random_files_with_structure(scratches_folder, test_images_folder, \"scratches\", 90)\nremaining_no_scratches = copy_random_files_with_structure(no_scratches_folder, test_images_folder, \"no_scratches\", 90)\n\n# Copy remaining images and maintain structure for `testdata`\ncopy_remaining_files_with_structure(scratches_folder, remaining_images_folder, remaining_scratches, \"scratches\")\ncopy_remaining_files_with_structure(no_scratches_folder, remaining_images_folder, remaining_no_scratches, \"no_scratches\")\n\nprint(\"Files copied successfully with folder structure preserved for both `binclas` and `testdata`!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T17:28:17.192923Z","iopub.execute_input":"2024-12-10T17:28:17.193279Z","iopub.status.idle":"2024-12-10T17:29:10.907656Z","shell.execute_reply.started":"2024-12-10T17:28:17.193250Z","shell.execute_reply":"2024-12-10T17:29:10.906708Z"}},"outputs":[{"name":"stdout","text":"Files copied successfully with folder structure preserved for both `binclas` and `testdata`!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nimport json\nfrom datetime import datetime\nimport logging\nimport sys\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nimport open_clip\nfrom torch.optim.lr_scheduler import MultiStepLR\nimport yaml\nimport argparse\n\n##################\n# Configuration for Binary Classification\n##################\n\n# Single category/attribute mapping for binary classification\n# \"defect_scratch\" has 2 classes: no_scratch(0) and scratch(1)\nCATEGORY_MAPPING = {\n    \"defect\": {\n        \"scratch\": \"class\"  # just a placeholder key\n    }\n}\n\ndef load_config(config_path):\n    with open(config_path, 'r') as f:\n        return yaml.safe_load(f)\n\ndef setup_logging(config):\n    \"\"\"Set up logging configuration\"\"\"\n    os.makedirs(config['paths']['log_dir'], exist_ok=True)\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = os.path.join(config['paths']['log_dir'], f'vith14_binary_class_{timestamp}.log')\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s | %(levelname)s | %(message)s',\n        handlers=[\n            logging.FileHandler(log_file),\n            logging.StreamHandler(sys.stdout)\n        ]\n    )\n    return logging.getLogger(__name__)\n\n##################\n# Dataset\n##################\n\nclass BinaryClassificationDataset(Dataset):\n    def __init__(self, root_dir, clip_preprocess_train, clip_preprocess_val, train=True):\n        \"\"\"\n        root_dir structure:\n         root_dir/\n           scratches/\n             img_1.jpg\n             img_2.jpg\n           no_scratches/\n             img_a.jpg\n             img_b.jpg\n\n        We'll assign:\n        - scratch: label=1\n        - no_scratch: label=0\n        \"\"\"\n        self.root_dir = root_dir\n        self.classes = [\"no_scratches\", \"scratches\"]  # order defines labeling: no_scratches=0, scratches=1\n        self.filepaths = []\n        self.labels = []\n        self.train = train\n\n        for class_idx, class_name in enumerate(self.classes):\n            class_dir = os.path.join(root_dir, class_name)\n            for fname in os.listdir(class_dir):\n                if fname.lower().endswith(('.jpg', '.png', '.jpeg')):\n                    self.filepaths.append(os.path.join(class_dir, fname))\n                    self.labels.append(class_idx)\n        \n        self.clip_preprocess_train = clip_preprocess_train\n        self.clip_preprocess_val = clip_preprocess_val\n\n    def __len__(self):\n        return len(self.filepaths)\n\n    def __getitem__(self, idx):\n        image_path = self.filepaths[idx]\n        label = self.labels[idx]\n        category = \"defect\"  # a single category for all samples\n\n        image = Image.open(image_path).convert('RGB')\n        if self.train:\n            image = self.clip_preprocess_train(image)\n        else:\n            image = self.clip_preprocess_val(image)\n\n        # We have a single attribute \"scratch\"\n        # Our target dict: {\"defect_scratch\": label}\n        targets = {\"defect_scratch\": label}\n\n        return image, category, targets\n\n##################\n# Collate Function\n##################\ndef custom_collate_fn(batch):\n    images = torch.stack([item[0] for item in batch])\n    categories = [item[1] for item in batch]\n\n    # Single attribute target\n    targets = {\"defect_scratch\": torch.tensor([item[2][\"defect_scratch\"] for item in batch], dtype=torch.long)}\n\n    return images, categories, targets\n\n##################\n# Model\n##################\n\nclass CategoryAwareAttributePredictor(nn.Module):\n    def __init__(self, clip_dim=768, category_attributes=None, attribute_dims=None, hidden_dim=512, dropout_rate=0.2, num_hidden_layers=1):\n        super(CategoryAwareAttributePredictor, self).__init__()\n        \n        self.category_attributes = category_attributes\n        self.attribute_predictors = nn.ModuleDict()\n        \n        for category, attributes in category_attributes.items():\n            for attr_name in attributes.keys():\n                key = f\"{category}_{attr_name}\"\n                if key in attribute_dims:\n                    layers = []\n                    # Input layer\n                    layers.append(nn.Linear(clip_dim, hidden_dim))\n                    layers.append(nn.LayerNorm(hidden_dim))\n                    layers.append(nn.ReLU())\n                    layers.append(nn.Dropout(dropout_rate))\n                    \n                    # Additional hidden layers\n                    current_dim = hidden_dim\n                    for _ in range(num_hidden_layers - 1):\n                        layers.append(nn.Linear(current_dim, current_dim // 2))\n                        layers.append(nn.LayerNorm(current_dim // 2))\n                        layers.append(nn.ReLU())\n                        layers.append(nn.Dropout(dropout_rate))\n                        current_dim = current_dim // 2\n\n                    # Output layer\n                    layers.append(nn.Linear(current_dim, attribute_dims[key]))\n                    \n                    self.attribute_predictors[key] = nn.Sequential(*layers)\n    \n    def forward(self, clip_features, category):\n        results = {}\n        category_attrs = self.category_attributes[category]\n        clip_features = clip_features.float()\n        \n        for attr_name in category_attrs.keys():\n            key = f\"{category}_{attr_name}\"\n            if key in self.attribute_predictors:\n                results[key] = self.attribute_predictors[key](clip_features)\n        \n        return results\n\n##################\n# Training Function\n##################\n\ndef train_model(\n        clip_model, \n        model, \n        train_dataset,\n        train_loader, \n        device,  \n        clip_lr,\n        predictor_lr, \n        weight_decay, \n        beta1, \n        beta2, \n        hidden_dim, \n        dropout_rate, \n        num_hidden_layers, \n        milestones, \n        gamma, \n        logger, \n        checkpoint_dir,\n        num_epochs=10, \n        accumulation_steps=2\n    ):\n\n    logger.info(\"Starting training...\")\n    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n\n    optimizer = optim.AdamW(\n        [\n            {'params': clip_model.parameters(), 'lr': clip_lr},\n            {'params': model.parameters(), 'lr': predictor_lr, 'weight_decay': weight_decay}\n        ], \n        betas=(beta1, beta2)\n    )\n    scaler = torch.cuda.amp.GradScaler()\n\n    scheduler = MultiStepLR(optimizer=optimizer, milestones=milestones, gamma=gamma)\n    metrics_history = {'train_loss': [], 'train_acc': []}\n    \n    for epoch in range(num_epochs):\n        print(\"here\")\n        logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n        model.train()\n        clip_model.train()\n\n        train_loss = 0.0\n        correct = 0\n        total = 0\n        num_batches = 0\n\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n        # print(\"abbbtooo?\")\n        optimizer.zero_grad()\n\n        for batch_idx, (images, categories, targets) in enumerate(pbar):\n            # print(\"or yhaan?\")\n            images = images.to(device)\n            batch_size = images.size(0)\n\n            with torch.cuda.amp.autocast():\n                image_features = clip_model.encode_image(images)\n                # print(\"idhar kyaaa?\")\n                # For each sample we get predictions\n                # We have only one attribute: defect_scratch\n                predictions = model(image_features, categories[0])  # all belong to \"defect\"\n                pred_logits = predictions[\"defect_scratch\"]  # shape [batch_size, 2]\n\n                target_vals = targets[\"defect_scratch\"].to(device)\n                loss = criterion(pred_logits, target_vals)\n                loss = loss / accumulation_steps\n                # print(loss)\n\n            scaler.scale(loss).backward()\n\n            if (batch_idx + 1) % accumulation_steps == 0:\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n\n            # Metrics\n            train_loss += loss.item() * accumulation_steps\n            num_batches += 1\n\n            _, predicted = torch.max(pred_logits, 1)\n            correct += (predicted == target_vals).sum().item()\n            total += batch_size\n\n            pbar.set_postfix(loss=(train_loss/num_batches), acc=correct/total)\n\n        scheduler.step()\n\n        avg_loss = train_loss / num_batches\n        accuracy = correct / total\n        metrics_history['train_loss'].append(avg_loss)\n        metrics_history['train_acc'].append(accuracy)\n\n        logger.info(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}\")\n\n        # Save a checkpoint each epoch\n        os.makedirs(checkpoint_dir, exist_ok=True)\n        save_path = os.path.join(checkpoint_dir, f\"binary_checkpoint_epoch{epoch+1}.pth\")\n        torch.save({\n            'model_state_dict': model.state_dict(),\n            'clip_model_state_dict': clip_model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'metrics': metrics_history\n        }, save_path)\n\n    logger.info(\"Training completed.\")\n    return model, clip_model, metrics_history\n\n##################\n# Main\n##################\ndef main():\n    # Instead of using argparse, just directly use `config_data`\n    config = config_data\n\n    # Set up logging\n    logger = setup_logging(config)\n    print(\"helooooooooooooo\")\n    logger.info(\"Starting training with provided config\")\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"imhere22222222\")\n    # Create CLIP model\n    clip_model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n        config['model']['name'],\n        pretrained=config['model']['pretrained'],\n        device=device\n    )\n    print(\"aayakyaaa\")\n    clip_model = clip_model.float()\n\n    # Prepare dataset\n    train_dir = os.path.join(config['paths']['data_dir'])\n    train_dataset = BinaryClassificationDataset(\n        root_dir=train_dir,\n        clip_preprocess_train=preprocess_train,\n        clip_preprocess_val=preprocess_val,\n        train=True\n    )\n    print(\"aayakyaa222\")\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['training']['batch_size'],\n        shuffle=True,\n        num_workers=8,\n        pin_memory=True,\n        collate_fn=custom_collate_fn\n    )\n    print(\"haan bhai\")\n    # Define attribute_dims for binary classification\n    attribute_dims = {\"defect_scratch\": 2}\n\n    model = CategoryAwareAttributePredictor(\n        clip_dim=config['model']['clip_dim'],\n        category_attributes=CATEGORY_MAPPING,\n        attribute_dims=attribute_dims,\n        hidden_dim=config['model']['hidden_dim'][0],\n        dropout_rate=config['model']['dropout_rate'][0],\n        num_hidden_layers=config['model']['num_hidden_layers'][0]\n    ).to(device)\n    print(\"abtobol\")\n    model, clip_model, metrics = train_model(\n        clip_model=clip_model,\n        model=model,\n        train_dataset=train_dataset,\n        train_loader=train_loader,\n        device=device,\n        clip_lr=config['optimizer']['clip_lr'][0],\n        predictor_lr=config['optimizer']['predictor_lr'][0],\n        weight_decay=config['optimizer']['weight_decay'][0],\n        beta1=config['optimizer']['beta1'][0],\n        beta2=config['optimizer']['beta2'][0],\n        hidden_dim=config['model']['hidden_dim'][0],\n        dropout_rate=config['model']['dropout_rate'][0],\n        num_hidden_layers=config['model']['num_hidden_layers'][0],\n        gamma=config['scheduler']['gamma'],\n        milestones=config['scheduler']['milestones'],\n        logger=logger,\n        checkpoint_dir=config['paths']['checkpoint_dir'],\n        num_epochs=config['training']['num_epochs'],\n        accumulation_steps=config['training']['accumulation_steps']\n    )\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T18:03:22.652373Z","iopub.execute_input":"2024-12-10T18:03:22.652727Z","iopub.status.idle":"2024-12-10T18:03:22.682606Z","shell.execute_reply.started":"2024-12-10T18:03:22.652694Z","shell.execute_reply":"2024-12-10T18:03:22.681741Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T18:03:23.407220Z","iopub.execute_input":"2024-12-10T18:03:23.407539Z","iopub.status.idle":"2024-12-10T18:11:45.381788Z","shell.execute_reply.started":"2024-12-10T18:03:23.407509Z","shell.execute_reply":"2024-12-10T18:11:45.380230Z"}},"outputs":[{"name":"stdout","text":"helooooooooooooo\nimhere22222222\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/tmp/ipykernel_23/4039194646.py:203: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"aayakyaaa\naayakyaa222\nhaan bhai\nabtobol\nhere\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:   0%|          | 0/1166 [00:00<?, ?it/s]/tmp/ipykernel_23/4039194646.py:228: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEpoch 1: 100%|██████████| 1166/1166 [01:52<00:00, 10.36it/s, acc=0.927, loss=0.204]\n","output_type":"stream"},{"name":"stdout","text":"here\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 1166/1166 [01:51<00:00, 10.48it/s, acc=0.968, loss=0.102]\n","output_type":"stream"},{"name":"stdout","text":"here\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 1166/1166 [01:51<00:00, 10.43it/s, acc=0.981, loss=0.0641]\n","output_type":"stream"},{"name":"stdout","text":"here\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 1166/1166 [01:51<00:00, 10.48it/s, acc=0.988, loss=0.042] \n","output_type":"stream"},{"name":"stdout","text":"here\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  34%|███▍      | 397/1166 [00:38<01:14, 10.33it/s, acc=0.992, loss=0.0297]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[31], line 334\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    325\u001b[0m model \u001b[38;5;241m=\u001b[39m CategoryAwareAttributePredictor(\n\u001b[1;32m    326\u001b[0m     clip_dim\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip_dim\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    327\u001b[0m     category_attributes\u001b[38;5;241m=\u001b[39mCATEGORY_MAPPING,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m     num_hidden_layers\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_hidden_layers\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    332\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabtobol\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 334\u001b[0m model, clip_model, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclip_lr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictor_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictor_lr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeta1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeta2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropout_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_hidden_layers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscheduler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgamma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmilestones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscheduler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmilestones\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpaths\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoint_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccumulation_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[31], line 249\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(clip_model, model, train_dataset, train_loader, device, clip_lr, predictor_lr, weight_decay, beta1, beta2, hidden_dim, dropout_rate, num_hidden_layers, milestones, gamma, logger, checkpoint_dir, num_epochs, accumulation_steps)\u001b[0m\n\u001b[1;32m    246\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Metrics\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m accumulation_steps\n\u001b[1;32m    250\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    252\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(pred_logits, \u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":32},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom PIL import Image\nimport open_clip\nimport yaml\nimport os\n\n##################\n# CATEGORY_MAPPING\n##################\nCATEGORY_MAPPING = {\n    \"defect\": {\n        \"scratch\": \"class\"\n    }\n}\n\n##################\n# CategoryAwareAttributePredictor\n##################\nclass CategoryAwareAttributePredictor(nn.Module):\n    def __init__(self, clip_dim=768, category_attributes=None, attribute_dims=None, hidden_dim=512, dropout_rate=0.2, num_hidden_layers=1):\n        super(CategoryAwareAttributePredictor, self).__init__()\n        \n        self.category_attributes = category_attributes\n        self.attribute_predictors = nn.ModuleDict()\n        \n        for category, attributes in category_attributes.items():\n            for attr_name in attributes.keys():\n                key = f\"{category}_{attr_name}\"\n                if key in attribute_dims:\n                    layers = []\n                    # Input layer\n                    layers.append(nn.Linear(clip_dim, hidden_dim))\n                    layers.append(nn.LayerNorm(hidden_dim))\n                    layers.append(nn.ReLU())\n                    layers.append(nn.Dropout(dropout_rate))\n                    \n                    # Additional hidden layers\n                    current_dim = hidden_dim\n                    for _ in range(num_hidden_layers - 1):\n                        layers.append(nn.Linear(current_dim, current_dim // 2))\n                        layers.append(nn.LayerNorm(current_dim // 2))\n                        layers.append(nn.ReLU())\n                        layers.append(nn.Dropout(dropout_rate))\n                        current_dim = current_dim // 2\n\n                    # Output layer\n                    layers.append(nn.Linear(current_dim, attribute_dims[key]))\n                    \n                    self.attribute_predictors[key] = nn.Sequential(*layers)\n    \n    def forward(self, clip_features, category):\n        results = {}\n        category_attrs = self.category_attributes[category]\n        clip_features = clip_features.float()\n        \n        for attr_name in category_attrs.keys():\n            key = f\"{category}_{attr_name}\"\n            if key in self.attribute_predictors:\n                results[key] = self.attribute_predictors[key](clip_features)\n        \n        return results\n\n##################\n# Helper Functions\n##################\n\ndef load_config(config_path):\n    with open(config_path, 'r') as f:\n        return yaml.safe_load(f)\n\ndef load_models(config, checkpoint_path, device):\n    # Create CLIP model and transforms\n    clip_model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n        config['model']['name'],\n        pretrained=config['model']['pretrained'],\n        device=device\n    )\n    clip_model = clip_model.float()\n    \n    # Define attribute_dims (binary classification: 2 classes)\n    attribute_dims = {\"defect_scratch\": 2}\n    \n    model = CategoryAwareAttributePredictor(\n        clip_dim=config['model']['clip_dim'],\n        category_attributes=CATEGORY_MAPPING,\n        attribute_dims=attribute_dims,\n        hidden_dim=config['model']['hidden_dim'][0],\n        dropout_rate=config['model']['dropout_rate'][0],\n        num_hidden_layers=config['model']['num_hidden_layers'][0]\n    ).to(device)\n\n    # Load checkpoint\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    clip_model.load_state_dict(checkpoint['clip_model_state_dict'])\n\n    model.eval()\n    clip_model.eval()\n    \n    return clip_model, model, preprocess_val\n\ndef infer_image(clip_model, model, preprocess, image_path, device):\n    image = Image.open(image_path).convert('RGB')\n    image_tensor = preprocess(image).unsqueeze(0).to(device)\n\n    category = \"defect\"  # known from training\n    \n    with torch.no_grad():\n        image_features = clip_model.encode_image(image_tensor)\n        predictions = model(image_features, category)\n        logits = predictions[\"defect_scratch\"]  # shape [1, 2]\n        probs = F.softmax(logits, dim=1)\n        pred_class = torch.argmax(probs, dim=1).item()\n\n        class_names = [\"no_scratches\", \"scratches\"]\n        pred_label = class_names[pred_class]\n        \n        return pred_label, probs.cpu().numpy()\n\n##################\n# Example usage (adjust paths as needed)\n##################\nif __name__ == \"__main__\":\n    # Path to config and checkpoint\n    config_path = \"/kaggle/working/config.yml\"\n    checkpoint_path = \"/kaggle/working/binary_checkpoint_epoch4.pth\"\n    test_image_path = \"/kaggle/working/testdata/scratches/Code01447.png\"\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    config = load_config(config_path)\n\n    # Load models\n    clip_model, model, preprocess_val = load_models(config, checkpoint_path, device)\n\n    # Run inference on a test image\n    pred_label, probs = infer_image(clip_model, model, preprocess_val, test_image_path, device)\n    print(f\"Predicted label: {pred_label}, Probabilities: {probs}\")\n\n# if __name__ == \"__main__\":\n#     # Example usage:\n#     # Provide the path to your config and a checkpoint file\n#     config_path = \"/kaggle/working/config.yml\"\n#     checkpoint_path = \"/kaggle/working/binary_checkpoint_epoch7.pth\"\n#     test_image_path = \"/kaggle/input/binnary-classification/scratches/03_08_2024_17_12_41.304965_classifier_input.png\"\n\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     config = load_config(config_path)\n#     clip_model, model, preprocess_val = load_models(config, checkpoint_path, device)\n\n#     pred_label, probs = infer_image(clip_model, model, preprocess_val, test_image_path, device)\n#     print(f\"Predicted label: {pred_label}, Probabilities: {probs}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T18:12:08.525010Z","iopub.execute_input":"2024-12-10T18:12:08.525329Z","iopub.status.idle":"2024-12-10T18:12:13.231804Z","shell.execute_reply.started":"2024-12-10T18:12:08.525302Z","shell.execute_reply":"2024-12-10T18:12:13.230887Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/2747002733.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Predicted label: scratches, Probabilities: [[0.00258269 0.99741733]]\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support\n\ndef evaluate_model(clip_model, model, preprocess, test_folder, device, num_samples=150):\n    class_names = [\"no_scratches\", \"scratches\"]\n    \n    # Collect all test image paths and labels\n    test_images = []\n    labels = []\n    for class_name in class_names:\n        class_folder = os.path.join(test_folder, class_name)\n        for file_name in os.listdir(class_folder):\n            if file_name.endswith(('.png', '.jpg', '.jpeg')):\n                test_images.append(os.path.join(class_folder, file_name))\n                labels.append(class_name)\n    \n    # Randomly select a subset of images for evaluation\n    selected_indices = random.sample(range(len(test_images)), min(num_samples, len(test_images)))\n    selected_images = [test_images[i] for i in selected_indices]\n    selected_labels = [labels[i] for i in selected_indices]\n    \n    # Perform inference on the selected images\n    preds = []\n    for image_path in selected_images:\n        pred_label, _ = infer_image(clip_model, model, preprocess, image_path, device)\n        preds.append(pred_label)\n    \n    # Convert class names to binary labels\n    label_map = {name: idx for idx, name in enumerate(class_names)}\n    y_true = [label_map[label] for label in selected_labels]\n    y_pred = [label_map[pred] for pred in preds]\n    \n    # Calculate precision, recall, and F1 score for each class\n    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None, labels=[0, 1])\n    \n    # precision, recall, f1 now are arrays with metrics for each class in order: class 0, class 1\n    # If you want the overall metrics (macro-average), you can also compute them:\n    # macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n    \n    return precision, recall, f1, support, class_names\n\nif __name__ == \"__main__\":\n    config_path = \"/kaggle/working/config.yml\"\n    checkpoint_path = \"/kaggle/working/binary_checkpoint_epoch4.pth\"\n    test_folder = \"/kaggle/working/testdata\"\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    config = load_config(config_path)\n\n    # Load models\n    clip_model, model, preprocess_val = load_models(config, checkpoint_path, device)\n\n    # Evaluate the model\n    precision, recall, f1, support, class_names = evaluate_model(clip_model, model, preprocess_val, test_folder, device, num_samples=100)\n    \n    for i, class_name in enumerate(class_names):\n        print(f\"Class: {class_name}\")\n        print(f\"  Precision: {precision[i]:.4f}\")\n        print(f\"  Recall:    {recall[i]:.4f}\")\n        print(f\"  F1 Score:  {f1[i]:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T18:12:15.786319Z","iopub.execute_input":"2024-12-10T18:12:15.786665Z","iopub.status.idle":"2024-12-10T18:12:22.199788Z","shell.execute_reply.started":"2024-12-10T18:12:15.786634Z","shell.execute_reply":"2024-12-10T18:12:22.198853Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/2747002733.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Class: no_scratches\n  Precision: 0.9643\n  Recall:    0.9759\n  F1 Score:  0.9701\nClass: scratches\n  Precision: 0.8750\n  Recall:    0.8235\n  F1 Score:  0.8485\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import open_clip\n\navailable_models = open_clip.list_pretrained()\nfor model_name, pretrained_name in available_models:\n    print(f\"Model: {model_name}, Pretrained: {pretrained_name}\")\n\nClass: no_scratches\n  Precision: 0.9390\n  Recall:    0.9625\n  F1 Score:  0.9506\nClass: scratches\n  Precision: 0.8333\n  Recall:    0.7500\n  F1 Score:  0.7895","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T17:24:44.822643Z","iopub.execute_input":"2024-12-10T17:24:44.823455Z","iopub.status.idle":"2024-12-10T17:24:52.814167Z","shell.execute_reply.started":"2024-12-10T17:24:44.823417Z","shell.execute_reply":"2024-12-10T17:24:52.813300Z"}},"outputs":[{"name":"stdout","text":"Model: RN50, Pretrained: openai\nModel: RN50, Pretrained: yfcc15m\nModel: RN50, Pretrained: cc12m\nModel: RN101, Pretrained: openai\nModel: RN101, Pretrained: yfcc15m\nModel: RN50x4, Pretrained: openai\nModel: RN50x16, Pretrained: openai\nModel: RN50x64, Pretrained: openai\nModel: ViT-B-32, Pretrained: openai\nModel: ViT-B-32, Pretrained: laion400m_e31\nModel: ViT-B-32, Pretrained: laion400m_e32\nModel: ViT-B-32, Pretrained: laion2b_e16\nModel: ViT-B-32, Pretrained: laion2b_s34b_b79k\nModel: ViT-B-32, Pretrained: datacomp_xl_s13b_b90k\nModel: ViT-B-32, Pretrained: datacomp_m_s128m_b4k\nModel: ViT-B-32, Pretrained: commonpool_m_clip_s128m_b4k\nModel: ViT-B-32, Pretrained: commonpool_m_laion_s128m_b4k\nModel: ViT-B-32, Pretrained: commonpool_m_image_s128m_b4k\nModel: ViT-B-32, Pretrained: commonpool_m_text_s128m_b4k\nModel: ViT-B-32, Pretrained: commonpool_m_basic_s128m_b4k\nModel: ViT-B-32, Pretrained: commonpool_m_s128m_b4k\nModel: ViT-B-32, Pretrained: datacomp_s_s13m_b4k\nModel: ViT-B-32, Pretrained: commonpool_s_clip_s13m_b4k\nModel: ViT-B-32, Pretrained: commonpool_s_laion_s13m_b4k\nModel: ViT-B-32, Pretrained: commonpool_s_image_s13m_b4k\nModel: ViT-B-32, Pretrained: commonpool_s_text_s13m_b4k\nModel: ViT-B-32, Pretrained: commonpool_s_basic_s13m_b4k\nModel: ViT-B-32, Pretrained: commonpool_s_s13m_b4k\nModel: ViT-B-32, Pretrained: metaclip_400m\nModel: ViT-B-32, Pretrained: metaclip_fullcc\nModel: ViT-B-32-256, Pretrained: datacomp_s34b_b86k\nModel: ViT-B-16, Pretrained: openai\nModel: ViT-B-16, Pretrained: laion400m_e31\nModel: ViT-B-16, Pretrained: laion400m_e32\nModel: ViT-B-16, Pretrained: laion2b_s34b_b88k\nModel: ViT-B-16, Pretrained: datacomp_xl_s13b_b90k\nModel: ViT-B-16, Pretrained: datacomp_l_s1b_b8k\nModel: ViT-B-16, Pretrained: commonpool_l_clip_s1b_b8k\nModel: ViT-B-16, Pretrained: commonpool_l_laion_s1b_b8k\nModel: ViT-B-16, Pretrained: commonpool_l_image_s1b_b8k\nModel: ViT-B-16, Pretrained: commonpool_l_text_s1b_b8k\nModel: ViT-B-16, Pretrained: commonpool_l_basic_s1b_b8k\nModel: ViT-B-16, Pretrained: commonpool_l_s1b_b8k\nModel: ViT-B-16, Pretrained: dfn2b\nModel: ViT-B-16, Pretrained: metaclip_400m\nModel: ViT-B-16, Pretrained: metaclip_fullcc\nModel: ViT-B-16-plus-240, Pretrained: laion400m_e31\nModel: ViT-B-16-plus-240, Pretrained: laion400m_e32\nModel: ViT-L-14, Pretrained: openai\nModel: ViT-L-14, Pretrained: laion400m_e31\nModel: ViT-L-14, Pretrained: laion400m_e32\nModel: ViT-L-14, Pretrained: laion2b_s32b_b82k\nModel: ViT-L-14, Pretrained: datacomp_xl_s13b_b90k\nModel: ViT-L-14, Pretrained: commonpool_xl_clip_s13b_b90k\nModel: ViT-L-14, Pretrained: commonpool_xl_laion_s13b_b90k\nModel: ViT-L-14, Pretrained: commonpool_xl_s13b_b90k\nModel: ViT-L-14, Pretrained: metaclip_400m\nModel: ViT-L-14, Pretrained: metaclip_fullcc\nModel: ViT-L-14, Pretrained: dfn2b\nModel: ViT-L-14-336, Pretrained: openai\nModel: ViT-H-14, Pretrained: laion2b_s32b_b79k\nModel: ViT-H-14, Pretrained: metaclip_fullcc\nModel: ViT-H-14, Pretrained: dfn5b\nModel: ViT-H-14-378, Pretrained: dfn5b\nModel: ViT-g-14, Pretrained: laion2b_s12b_b42k\nModel: ViT-g-14, Pretrained: laion2b_s34b_b88k\nModel: ViT-bigG-14, Pretrained: laion2b_s39b_b160k\nModel: ViT-bigG-14, Pretrained: metaclip_fullcc\nModel: roberta-ViT-B-32, Pretrained: laion2b_s12b_b32k\nModel: xlm-roberta-base-ViT-B-32, Pretrained: laion5b_s13b_b90k\nModel: xlm-roberta-large-ViT-H-14, Pretrained: frozen_laion5b_s13b_b90k\nModel: convnext_base, Pretrained: laion400m_s13b_b51k\nModel: convnext_base_w, Pretrained: laion2b_s13b_b82k\nModel: convnext_base_w, Pretrained: laion2b_s13b_b82k_augreg\nModel: convnext_base_w, Pretrained: laion_aesthetic_s13b_b82k\nModel: convnext_base_w_320, Pretrained: laion_aesthetic_s13b_b82k\nModel: convnext_base_w_320, Pretrained: laion_aesthetic_s13b_b82k_augreg\nModel: convnext_large_d, Pretrained: laion2b_s26b_b102k_augreg\nModel: convnext_large_d_320, Pretrained: laion2b_s29b_b131k_ft\nModel: convnext_large_d_320, Pretrained: laion2b_s29b_b131k_ft_soup\nModel: convnext_xxlarge, Pretrained: laion2b_s34b_b82k_augreg\nModel: convnext_xxlarge, Pretrained: laion2b_s34b_b82k_augreg_rewind\nModel: convnext_xxlarge, Pretrained: laion2b_s34b_b82k_augreg_soup\nModel: coca_ViT-B-32, Pretrained: laion2b_s13b_b90k\nModel: coca_ViT-B-32, Pretrained: mscoco_finetuned_laion2b_s13b_b90k\nModel: coca_ViT-L-14, Pretrained: laion2b_s13b_b90k\nModel: coca_ViT-L-14, Pretrained: mscoco_finetuned_laion2b_s13b_b90k\nModel: EVA01-g-14, Pretrained: laion400m_s11b_b41k\nModel: EVA01-g-14-plus, Pretrained: merged2b_s11b_b114k\nModel: EVA02-B-16, Pretrained: merged2b_s8b_b131k\nModel: EVA02-L-14, Pretrained: merged2b_s4b_b131k\nModel: EVA02-L-14-336, Pretrained: merged2b_s6b_b61k\nModel: EVA02-E-14, Pretrained: laion2b_s4b_b115k\nModel: EVA02-E-14-plus, Pretrained: laion2b_s9b_b144k\nModel: ViT-B-16-SigLIP, Pretrained: webli\nModel: ViT-B-16-SigLIP-256, Pretrained: webli\nModel: ViT-B-16-SigLIP-i18n-256, Pretrained: webli\nModel: ViT-B-16-SigLIP-384, Pretrained: webli\nModel: ViT-B-16-SigLIP-512, Pretrained: webli\nModel: ViT-L-16-SigLIP-256, Pretrained: webli\nModel: ViT-L-16-SigLIP-384, Pretrained: webli\nModel: ViT-SO400M-14-SigLIP, Pretrained: webli\nModel: ViT-SO400M-16-SigLIP-i18n-256, Pretrained: webli\nModel: ViT-SO400M-14-SigLIP-378, Pretrained: webli\nModel: ViT-SO400M-14-SigLIP-384, Pretrained: webli\nModel: ViT-L-14-CLIPA, Pretrained: datacomp1b\nModel: ViT-L-14-CLIPA-336, Pretrained: datacomp1b\nModel: ViT-H-14-CLIPA, Pretrained: datacomp1b\nModel: ViT-H-14-CLIPA-336, Pretrained: laion2b\nModel: ViT-H-14-CLIPA-336, Pretrained: datacomp1b\nModel: ViT-bigG-14-CLIPA, Pretrained: datacomp1b\nModel: ViT-bigG-14-CLIPA-336, Pretrained: datacomp1b\nModel: nllb-clip-base, Pretrained: v1\nModel: nllb-clip-large, Pretrained: v1\nModel: nllb-clip-base-siglip, Pretrained: v1\nModel: nllb-clip-base-siglip, Pretrained: mrl\nModel: nllb-clip-large-siglip, Pretrained: v1\nModel: nllb-clip-large-siglip, Pretrained: mrl\nModel: MobileCLIP-S1, Pretrained: datacompdr\nModel: MobileCLIP-S2, Pretrained: datacompdr\nModel: MobileCLIP-B, Pretrained: datacompdr\nModel: MobileCLIP-B, Pretrained: datacompdr_lt\nModel: ViTamin-S, Pretrained: datacomp1b\nModel: ViTamin-S-LTT, Pretrained: datacomp1b\nModel: ViTamin-B, Pretrained: datacomp1b\nModel: ViTamin-B-LTT, Pretrained: datacomp1b\nModel: ViTamin-L, Pretrained: datacomp1b\nModel: ViTamin-L-256, Pretrained: datacomp1b\nModel: ViTamin-L-336, Pretrained: datacomp1b\nModel: ViTamin-L-384, Pretrained: datacomp1b\nModel: ViTamin-L2, Pretrained: datacomp1b\nModel: ViTamin-L2-256, Pretrained: datacomp1b\nModel: ViTamin-L2-336, Pretrained: datacomp1b\nModel: ViTamin-L2-384, Pretrained: datacomp1b\nModel: ViTamin-XL-256, Pretrained: datacomp1b\nModel: ViTamin-XL-336, Pretrained: datacomp1b\nModel: ViTamin-XL-384, Pretrained: datacomp1b\nModel: RN50-quickgelu, Pretrained: openai\nModel: RN50-quickgelu, Pretrained: yfcc15m\nModel: RN50-quickgelu, Pretrained: cc12m\nModel: RN101-quickgelu, Pretrained: openai\nModel: RN101-quickgelu, Pretrained: yfcc15m\nModel: RN50x4-quickgelu, Pretrained: openai\nModel: RN50x16-quickgelu, Pretrained: openai\nModel: RN50x64-quickgelu, Pretrained: openai\nModel: ViT-B-32-quickgelu, Pretrained: openai\nModel: ViT-B-32-quickgelu, Pretrained: laion400m_e31\nModel: ViT-B-32-quickgelu, Pretrained: laion400m_e32\nModel: ViT-B-32-quickgelu, Pretrained: metaclip_400m\nModel: ViT-B-32-quickgelu, Pretrained: metaclip_fullcc\nModel: ViT-B-16-quickgelu, Pretrained: openai\nModel: ViT-B-16-quickgelu, Pretrained: dfn2b\nModel: ViT-B-16-quickgelu, Pretrained: metaclip_400m\nModel: ViT-B-16-quickgelu, Pretrained: metaclip_fullcc\nModel: ViT-L-14-quickgelu, Pretrained: openai\nModel: ViT-L-14-quickgelu, Pretrained: metaclip_400m\nModel: ViT-L-14-quickgelu, Pretrained: metaclip_fullcc\nModel: ViT-L-14-quickgelu, Pretrained: dfn2b\nModel: ViT-L-14-336-quickgelu, Pretrained: openai\nModel: ViT-H-14-quickgelu, Pretrained: metaclip_fullcc\nModel: ViT-H-14-quickgelu, Pretrained: dfn5b\nModel: ViT-H-14-378-quickgelu, Pretrained: dfn5b\nModel: ViT-bigG-14-quickgelu, Pretrained: metaclip_fullcc\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}